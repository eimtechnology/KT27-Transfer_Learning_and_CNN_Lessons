{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Lesson 7: MobileNet-V2 Transfer Learning for Flower Classification\n",
    "\n",
    "## Overview\n",
    "Learn transfer learning with MobileNet-V2, a mobile-optimized CNN architecture designed for efficient inference on resource-constrained devices. Explore how depthwise separable convolutions and inverted residuals achieve excellent accuracy-to-efficiency ratios.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand depthwise separable convolutions and mobile optimization\n",
    "- Implement MobileNet-V2 transfer learning for flower classification\n",
    "- Analyze mobile deployment considerations and efficiency trade-offs\n",
    "- Compare final results across all 5 model architectures\n",
    "\n",
    "### Model Quick Facts\n",
    "- **Architecture**: MobileNet-V2 (mobile-optimized CNN with 3.5M parameters)\n",
    "- **Pre-training**: ImageNet dataset (1.2M images, 1000 classes)\n",
    "- **Key Innovation**: Depthwise separable convolutions + inverted residuals\n",
    "- **Input Resolution**: 224Ã—224 (standard resolution, efficient processing)\n",
    "- **Expected Performance**: ~87% accuracy on Flowers102 (highest efficiency!)\n",
    "- **Mobile Advantage**: 8-9Ã— fewer parameters than ResNet, 3-5Ã— faster inference\n",
    "\n",
    "### Why MobileNet-V2?\n",
    "- **Smallest Model**: Only 3.5M parameters (vs 25.6M for ResNet50)\n",
    "- **Fastest Training**: ~8-12 minutes total training time\n",
    "- **Best Efficiency**: 24.9% accuracy per million parameters\n",
    "- **Mobile Optimization**: Designed specifically for mobile deployment\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Environment Setup and Library Imports\n",
    "\n",
    "### Why This Step Matters\n",
    "MobileNet-V2 is optimized for efficient inference:\n",
    "- **Lower Memory Usage**: 3.5M parameters vs 25.6M for ResNet50\n",
    "- **Faster Training**: Efficient architecture reduces training time\n",
    "- **Standard Resolution**: 224Ã—224 input (same as ResNet, no memory pressure)\n",
    "- **CPU Friendly**: Designed for mobile CPUs, works well on any hardware\n",
    "\n",
    "### Key Advantages for Development\n",
    "- **No GPU Required**: Can train effectively on CPU\n",
    "- **Larger Batch Size**: 32 works well due to lower memory usage\n",
    "- **Stable Training**: Mobile-optimized architecture is robust\n",
    "- **Fast Iteration**: Quick training allows rapid experimentation\n",
    "\n",
    "### MobileNet-V2 Architecture Principles\n",
    "- **Depthwise Separable Convolutions**: Separate spatial and channel operations\n",
    "- **Inverted Residuals**: Expand â†’ Depthwise â†’ Project workflow\n",
    "- **Linear Bottlenecks**: Prevent information loss in low-dimensional spaces\n",
    "- **Width Multiplier**: Scalable architecture (0.5Ã—, 0.75Ã—, 1.0Ã—)\n",
    "\n",
    "### Efficiency Comparison\n",
    "| Component | ResNet50 | EfficientNet-B0 | MobileNet-V2 |\n",
    "|-----------|----------|-----------------|--------------|\n",
    "| **Parameters** | 25.6M | 5.3M | **3.5M** |\n",
    "| **Memory** | High | Medium | **Low** |\n",
    "| **Training Time** | ~25-30min | ~10-15min | **~8-12min** |\n",
    "| **Mobile Score** | â­ | â­â­â­â­ | **â­â­â­â­â­** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Computer vision utilities\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Data handling and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for high-quality plots\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“¦ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ–¼ï¸ Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"ğŸ”¥ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"ğŸ MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Check MobileNet-V2 availability\n",
    "try:\n",
    "    test_model = models.mobilenet_v2(pretrained=False)\n",
    "    print(\"âœ… MobileNet-V2 available!\")\n",
    "    print(f\"ğŸ”¢ Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "    del test_model  # Clean up\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MobileNet-V2 not available: {e}\")\n",
    "    print(\"ğŸ’¡ Please update PyTorch/torchvision to latest version\")\n",
    "\n",
    "# Set device (MobileNet-V2 works well on CPU too!)\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"ğŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"ğŸš€ Using Apple MPS\")\n",
    "else:\n",
    "    print(\"ğŸ’» Using CPU (perfectly fine for MobileNet-V2!)\")\n",
    "\n",
    "print(f\"ğŸ¯ Device selected: {device}\")\n",
    "\n",
    "# Memory optimization (less critical for MobileNet-V2)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"ğŸ§¹ GPU memory cleared\")\n",
    "    print(f\"ğŸ’¾ GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Data Loading and Preprocessing\n",
    "\n",
    "### Dataset Overview\n",
    "- **Flowers102**: 102 flower categories, 8,189 images\n",
    "- **Split**: 1,020 training, 1,020 validation, 6,149 test images\n",
    "- **Image Size**: Variable, we'll resize to 224Ã—224 for MobileNet-V2\n",
    "\n",
    "### MobileNet-V2 Input Requirements\n",
    "Unlike our previous models, MobileNet-V2 uses **standard 224Ã—224 input**:\n",
    "\n",
    "**Comparison with Previous Models:**\n",
    "- **ResNet18/50**: 224Ã—224 (same as MobileNet-V2)\n",
    "- **EfficientNet-B0**: 224Ã—224 (same as MobileNet-V2)\n",
    "- **EfficientNet-B3**: 300Ã—300 (higher resolution)\n",
    "- **MobileNet-V2**: 224Ã—224 (efficient standard resolution)\n",
    "\n",
    "### Why 224Ã—224 is Optimal for Mobile\n",
    "- **Memory Efficient**: Standard resolution minimizes memory usage\n",
    "- **Fast Processing**: Optimized for mobile hardware\n",
    "- **Balanced Trade-off**: Good accuracy without excessive computation\n",
    "- **Hardware Friendly**: Fits well in mobile GPU/NPU memory\n",
    "\n",
    "### Data Augmentation Strategy\n",
    "**Mobile-Optimized Augmentation:**\n",
    "- **Minimal Augmentation**: Avoid complex augmentations that slow inference\n",
    "- **Standard Transformations**: Resize, crop, flip, normalize\n",
    "- **Efficient Pipeline**: Optimized for mobile deployment\n",
    "- **Consistency**: Same preprocessing for training and inference\n",
    "\n",
    "### Batch Size Considerations\n",
    "- **MobileNet-V2**: Can use batch_size=32 (efficient memory usage)\n",
    "- **EfficientNet-B3**: Required batch_size=24 (higher memory usage)\n",
    "- **Benefits**: Larger batch size â†’ better gradient estimates â†’ faster convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations for MobileNet-V2 (224Ã—224 input)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Slightly larger for crop\n",
    "    transforms.RandomCrop(224),     # Standard resolution\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Direct resize for validation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\"ğŸ“ Loading Flowers102 dataset...\")\n",
    "try:\n",
    "    train_dataset = torchvision.datasets.Flowers102(\n",
    "        root='./data', \n",
    "        split='train',\n",
    "        transform=train_transforms,\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = torchvision.datasets.Flowers102(\n",
    "        root='./data', \n",
    "        split='val',\n",
    "        transform=val_transforms,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.Flowers102(\n",
    "        root='./data', \n",
    "        split='test',\n",
    "        transform=val_transforms,\n",
    "        download=False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Training images: {len(train_dataset)}\")\n",
    "    print(f\"ğŸ“Š Validation images: {len(val_dataset)}\")\n",
    "    print(f\"ğŸ“Š Test images: {len(test_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure you have internet connection for first download\")\n",
    "\n",
    "# Create data loaders - MobileNet-V2 can handle larger batch sizes\n",
    "BATCH_SIZE = 32  # Efficient batch size for MobileNet-V2\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”„ Data loaders created with batch size: {BATCH_SIZE}\")\n",
    "print(f\"ğŸ“¦ Training batches: {len(train_loader)}\")\n",
    "print(f\"ğŸ“¦ Validation batches: {len(val_loader)}\")\n",
    "print(f\"ğŸ“¦ Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(f\"\\nğŸ’¾ Memory usage comparison:\")\n",
    "print(f\"   EfficientNet-B3 (300Ã—300): {24 * 3 * 300 * 300 / 1024**2:.1f}MB per batch\")\n",
    "print(f\"   MobileNet-V2 (224Ã—224): {32 * 3 * 224 * 224 / 1024**2:.1f}MB per batch\")\n",
    "\n",
    "# Visualize sample images\n",
    "def visualize_batch(data_loader, title):\n",
    "    \"\"\"Visualize a batch of images\"\"\"\n",
    "    batch_images, batch_labels = next(iter(data_loader))\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    batch_images = batch_images * std + mean\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(8):\n",
    "        img = batch_images[i].permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Class: {batch_labels[i].item()}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training samples\n",
    "print(\"\\nğŸ–¼ï¸ Sample training images:\")\n",
    "visualize_batch(train_loader, \"Training Images (224Ã—224, Mobile-Optimized)\")\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"\\nğŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Input Resolution: 224Ã—224 (efficient for mobile)\")\n",
    "print(f\"   Total Classes: 102\")\n",
    "print(f\"   Training Images: {len(train_dataset)}\")\n",
    "print(f\"   Validation Images: {len(val_dataset)}\")\n",
    "print(f\"   Test Images: {len(test_dataset)}\")\n",
    "print(f\"   Images per Class: ~{len(train_dataset) / 102:.1f} (train), ~{len(val_dataset) / 102:.1f} (val)\")\n",
    "\n",
    "print(f\"\\nğŸš€ Mobile Advantages:\")\n",
    "print(f\"   âœ… Standard resolution (224Ã—224) - mobile-friendly\")\n",
    "print(f\"   âœ… Efficient batch size (32) - better gradient estimates\")\n",
    "print(f\"   âœ… Lower memory usage - works on any device\")\n",
    "print(f\"   âœ… Fast data loading - optimized pipeline\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: MobileNet-V2 Model Setup and Architecture Analysis\n",
    "\n",
    "### MobileNet-V2 Architecture Deep Dive\n",
    "\n",
    "**Core Innovation: Depthwise Separable Convolutions**\n",
    "- **Standard Convolution**: 3Ã—3Ã—64Ã—128 = 73,728 parameters\n",
    "- **Depthwise Separable**: 3Ã—3Ã—64Ã—1 + 1Ã—1Ã—64Ã—128 = 8,768 parameters\n",
    "- **Reduction**: 88% fewer parameters!\n",
    "\n",
    "**Inverted Residual Blocks:**\n",
    "1. **Expansion**: 1Ã—1 conv increases channels (6Ã— expansion)\n",
    "2. **Depthwise**: 3Ã—3 depthwise conv with stride\n",
    "3. **Projection**: 1Ã—1 conv reduces channels back\n",
    "4. **Residual**: Skip connection when stride=1\n",
    "\n",
    "### Architecture Comparison\n",
    "\n",
    "| Component | ResNet50 | EfficientNet-B0 | MobileNet-V2 |\n",
    "|-----------|----------|-----------------|--------------|\n",
    "| **Parameters** | 25.6M | 5.3M | **3.5M** |\n",
    "| **Blocks** | Bottleneck | MBConv | Inverted Residual |\n",
    "| **Convolution** | Standard | Depthwise Sep | Depthwise Sep |\n",
    "| **Optimization** | Depth | Compound Scale | **Mobile Efficiency** |\n",
    "| **Target** | Server | Balanced | **Mobile/Edge** |\n",
    "\n",
    "### Model Scaling Strategy\n",
    "- **Width Multiplier**: Controls channel count (0.5Ã—, 0.75Ã—, 1.0Ã—)\n",
    "- **Resolution Multiplier**: Controls input resolution\n",
    "- **Depth Multiplier**: Controls number of layers\n",
    "\n",
    "### Transfer Learning Adaptation\n",
    "- **Pre-trained Backbone**: ImageNet features (1000 classes)\n",
    "- **Custom Classifier**: Replace final layer for 102 flower classes\n",
    "- **Feature Extraction**: Initial frozen training\n",
    "- **Fine-tuning**: End-to-end optimization\n",
    "\n",
    "### Expected Performance\n",
    "- **Parameters**: 3.5M (smallest in our comparison)\n",
    "- **Accuracy**: ~87% (good balance of accuracy/efficiency)\n",
    "- **Training Speed**: Fastest among all models\n",
    "- **Mobile Deployment**: Optimal for real-world applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNet-V2 model\n",
    "print(\"ğŸ—ï¸ Loading MobileNet-V2 model...\")\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Modify the classifier for 102 flower classes\n",
    "num_classes = 102\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… MobileNet-V2 model loaded successfully!\")\n",
    "print(f\"ğŸ“Š Total parameters: {total_params:,}\")\n",
    "print(f\"ğŸ¯ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"ğŸ“± Model size: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "\n",
    "# Analyze model architecture\n",
    "print(f\"\\nğŸ” MobileNet-V2 Architecture Analysis:\")\n",
    "print(f\"   Input Resolution: 224Ã—224\")\n",
    "print(f\"   Feature Extractor: Inverted Residual Blocks\")\n",
    "print(f\"   Classifier: Adaptive Avg Pool + Linear\")\n",
    "print(f\"   Output Classes: {num_classes}\")\n",
    "\n",
    "# Compare with other models\n",
    "print(f\"\\nğŸ“Š Model Comparison:\")\n",
    "print(f\"   ResNet18: 11.7M parameters\")\n",
    "print(f\"   ResNet50: 25.6M parameters\")\n",
    "print(f\"   EfficientNet-B0: 5.3M parameters\")\n",
    "print(f\"   EfficientNet-B3: 12.2M parameters\")\n",
    "print(f\"   MobileNet-V2: {total_params/1000000:.1f}M parameters â­\")\n",
    "\n",
    "# Visualize model architecture\n",
    "print(f\"\\nğŸ—ï¸ Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Analyze feature extractor layers\n",
    "print(f\"\\nğŸ”¬ Feature Extractor Analysis:\")\n",
    "feature_layers = 0\n",
    "for name, module in model.features.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        feature_layers += 1\n",
    "        \n",
    "print(f\"   Total Conv2d layers: {feature_layers}\")\n",
    "print(f\"   Inverted Residual blocks: 17\")\n",
    "print(f\"   Depthwise separable convs: Most operations\")\n",
    "print(f\"   Skip connections: When stride=1\")\n",
    "\n",
    "# Test forward pass\n",
    "print(f\"\\nğŸ§ª Testing forward pass...\")\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"âœ… Forward pass successful!\")\n",
    "    print(f\"   Input shape: {sample_input.shape}\")\n",
    "    print(f\"   Output shape: {sample_output.shape}\")\n",
    "    print(f\"   Output classes: {sample_output.shape[1]}\")\n",
    "\n",
    "# Memory usage analysis\n",
    "if device.type == 'cuda':\n",
    "    print(f\"\\nğŸ’¾ GPU Memory Usage:\")\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated(0)/1024**2:.1f}MB\")\n",
    "    print(f\"   Cached: {torch.cuda.memory_reserved(0)/1024**2:.1f}MB\")\n",
    "    \n",
    "print(f\"\\nğŸš€ MobileNet-V2 Advantages:\")\n",
    "print(f\"   âœ… Smallest model in our comparison\")\n",
    "print(f\"   âœ… Efficient depthwise separable convolutions\")\n",
    "print(f\"   âœ… Inverted residual blocks for better gradient flow\")\n",
    "print(f\"   âœ… Optimized for mobile deployment\")\n",
    "print(f\"   âœ… Fast training and inference\")\n",
    "print(f\"   âœ… Low memory footprint\")\n",
    "\n",
    "# Efficiency metrics\n",
    "print(f\"\\nğŸ“ˆ Efficiency Metrics:\")\n",
    "print(f\"   Parameters: {total_params:,}\")\n",
    "print(f\"   Model Size: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "print(f\"   Expected Accuracy: ~87%\")\n",
    "print(f\"   Efficiency: {87/(total_params/1000000):.1f}% per M params\")\n",
    "print(f\"   Training Time: ~8-12 minutes\")\n",
    "print(f\"   Mobile Score: â­â­â­â­â­\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Training Configuration and Optimization\n",
    "\n",
    "### Training Strategy Overview\n",
    "We'll use the same progressive training approach as other models:\n",
    "\n",
    "**Phase 1: Feature Extraction (Epochs 1-20)**\n",
    "- Freeze feature extractor layers\n",
    "- Train only classifier head\n",
    "- Learning rate: 0.001\n",
    "- Focus on adapting pre-trained features\n",
    "\n",
    "**Phase 2: Fine-tuning (Epochs 21-50)**\n",
    "- Unfreeze all layers\n",
    "- Lower learning rate: 0.0001\n",
    "- End-to-end optimization\n",
    "- Careful gradient flow\n",
    "\n",
    "### Mobile-Optimized Training Configuration\n",
    "\n",
    "**Optimizer Selection:**\n",
    "- **AdamW**: Adaptive learning with weight decay\n",
    "- **Weight Decay**: 0.01 (prevent overfitting)\n",
    "- **Efficient**: Works well with smaller models\n",
    "\n",
    "**Learning Rate Strategy:**\n",
    "- **Phase 1**: 0.001 (standard rate for classifier)\n",
    "- **Phase 2**: 0.0001 (careful fine-tuning)\n",
    "- **Scheduler**: StepLR with gamma=0.1\n",
    "\n",
    "**Batch Processing:**\n",
    "- **Batch Size**: 32 (efficient for MobileNet-V2)\n",
    "- **Gradient Accumulation**: Not needed due to efficient architecture\n",
    "- **Memory**: Lower usage allows larger batches\n",
    "\n",
    "### Why This Configuration Works for MobileNet-V2\n",
    "- **Efficient Architecture**: Fewer parameters â†’ faster convergence\n",
    "- **Stable Training**: Inverted residuals provide good gradient flow\n",
    "- **Lower Learning Rates**: Prevent degradation of mobile optimizations\n",
    "- **Regularization**: Dropout and weight decay prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer (AdamW works well for mobile architectures)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "print(f\"   Total Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Initial Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   Scheduler: StepLR (step_size=20, gamma=0.1)\")\n",
    "print(f\"   Loss Function: CrossEntropyLoss\")\n",
    "\n",
    "# Training phase configuration\n",
    "print(f\"\\nğŸ“š Training Phases:\")\n",
    "print(f\"   Phase 1 (Epochs 1-20): Feature extraction (frozen backbone)\")\n",
    "print(f\"   Phase 2 (Epochs 21-50): Fine-tuning (unfrozen backbone)\")\n",
    "\n",
    "# Mobile-specific optimizations\n",
    "print(f\"\\nğŸ“± Mobile Optimizations:\")\n",
    "print(f\"   âœ… Efficient batch size (32) for faster training\")\n",
    "print(f\"   âœ… Lower learning rates to preserve mobile optimizations\")\n",
    "print(f\"   âœ… AdamW optimizer for stable convergence\")\n",
    "print(f\"   âœ… Progressive training strategy\")\n",
    "\n",
    "# Memory efficiency analysis\n",
    "print(f\"\\nğŸ’¾ Memory Efficiency:\")\n",
    "print(f\"   Model Size: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "print(f\"   Batch Memory: ~{BATCH_SIZE * 3 * 224 * 224 * 4 / 1024**2:.1f}MB\")\n",
    "print(f\"   Total GPU Memory: ~{(total_params * 4 + BATCH_SIZE * 3 * 224 * 224 * 4) / 1024**2:.1f}MB\")\n",
    "\n",
    "# Helper functions for training\n",
    "def freeze_backbone(model):\n",
    "    \"\"\"Freeze feature extractor for phase 1\"\"\"\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Only classifier parameters are trainable\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"ğŸ”’ Backbone frozen. Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "def unfreeze_backbone(model):\n",
    "    \"\"\"Unfreeze all layers for phase 2\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"ğŸ”“ Backbone unfrozen. Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(f\"\\nğŸš€ Ready to start training!\")\n",
    "print(f\"   Expected training time: ~8-12 minutes\")\n",
    "print(f\"   Expected Phase 1 accuracy: ~80%\")\n",
    "print(f\"   Expected Phase 2 accuracy: ~87%\")\n",
    "print(f\"   Mobile efficiency: 24.9% per M params\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Progressive Training - Phase 1 (Feature Extraction)\n",
    "\n",
    "### Phase 1 Overview\n",
    "**Goal**: Adapt pre-trained ImageNet features for flower classification\n",
    "- **Strategy**: Freeze backbone, train only classifier\n",
    "- **Duration**: 20 epochs\n",
    "- **Learning Rate**: 0.001\n",
    "- **Expected Accuracy**: ~80%\n",
    "\n",
    "### Why Start with Feature Extraction?\n",
    "1. **Preserve Pre-trained Features**: ImageNet features are valuable for flower classification\n",
    "2. **Stable Training**: Avoid disrupting mobile optimizations immediately\n",
    "3. **Fast Convergence**: Classifier learns to use existing features efficiently\n",
    "4. **Baseline Establishment**: Good starting point for fine-tuning\n",
    "\n",
    "### Mobile-Specific Considerations\n",
    "- **Efficient Architecture**: MobileNet-V2 converges faster than larger models\n",
    "- **Lower Memory Usage**: Frozen backbone reduces memory requirements\n",
    "- **Stable Gradients**: Inverted residuals provide good gradient flow\n",
    "- **Preserved Optimizations**: Mobile-specific optimizations remain intact\n",
    "\n",
    "### Expected Training Behavior\n",
    "- **Early Epochs**: Rapid accuracy improvement\n",
    "- **Mid Training**: Steady convergence\n",
    "- **Final Epochs**: Plateau around 80% accuracy\n",
    "- **Loss Curve**: Smooth decrease with occasional fluctuations\n",
    "\n",
    "### Phase 1 Performance Targets\n",
    "- **Training Accuracy**: ~85%\n",
    "- **Validation Accuracy**: ~80%\n",
    "- **Training Time**: ~4-6 minutes\n",
    "- **Memory Usage**: Efficient due to frozen backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Feature Extraction Training (Epochs 1-20)\n",
    "print(\"ğŸš€ Starting Phase 1: Feature Extraction Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Freeze backbone for feature extraction\n",
    "freeze_backbone(model)\n",
    "\n",
    "# Initialize tracking variables\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "epoch_times = []\n",
    "\n",
    "# Best model tracking\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Phase 1 training loop\n",
    "phase1_start_time = time.time()\n",
    "\n",
    "for epoch in range(1, 21):  # Epochs 1-20\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch:2d}/20 | \"\n",
    "          f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping check (optional)\n",
    "    if epoch > 5 and val_acc < max(val_accuracies[:-1]) - 5:\n",
    "        print(f\"âš ï¸  Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "phase1_end_time = time.time()\n",
    "phase1_duration = phase1_end_time - phase1_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š Phase 1 Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ¯ Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"â±ï¸  Total Training Time: {phase1_duration:.1f} seconds ({phase1_duration/60:.1f} minutes)\")\n",
    "print(f\"âš¡ Average Time per Epoch: {phase1_duration/20:.1f} seconds\")\n",
    "print(f\"ğŸ† Expected vs Actual: ~80% vs {best_val_acc:.2f}%\")\n",
    "\n",
    "# Performance analysis\n",
    "phase1_final_train_acc = train_accuracies[-1]\n",
    "phase1_final_val_acc = val_accuracies[-1]\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Phase 1 Performance Analysis:\")\n",
    "print(f\"   Final Training Accuracy: {phase1_final_train_acc:.2f}%\")\n",
    "print(f\"   Final Validation Accuracy: {phase1_final_val_acc:.2f}%\")\n",
    "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Overfitting Gap: {phase1_final_train_acc - phase1_final_val_acc:.2f}%\")\n",
    "\n",
    "# Mobile efficiency metrics\n",
    "print(f\"\\nğŸ“± Mobile Efficiency Metrics:\")\n",
    "print(f\"   Training Speed: {len(train_dataset) / phase1_duration:.1f} images/second\")\n",
    "print(f\"   Parameter Efficiency: {best_val_acc / (total_params/1000000):.1f}% per M params\")\n",
    "print(f\"   Memory Efficiency: Low memory usage due to frozen backbone\")\n",
    "\n",
    "# Prepare for Phase 2\n",
    "print(f\"\\nğŸ”„ Preparing for Phase 2...\")\n",
    "print(f\"   Phase 1 baseline: {best_val_acc:.2f}%\")\n",
    "print(f\"   Phase 2 target: ~87% (+{87-best_val_acc:.1f}% improvement)\")\n",
    "print(f\"   Strategy: Unfreeze backbone, lower learning rate\")\n",
    "\n",
    "# Load best model from Phase 1\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"âœ… Loaded best model from Phase 1 (Val Acc: {best_val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for Phase 2: Fine-tuning!\")\n",
    "print(f\"   Expected improvement: ~{87-best_val_acc:.1f}%\")\n",
    "print(f\"   Expected final accuracy: ~87%\")\n",
    "print(f\"   Mobile advantages: Efficient training, fast convergence\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: Progressive Training - Phase 2 (Fine-tuning)\n",
    "\n",
    "### Phase 2 Overview\n",
    "**Goal**: End-to-end fine-tuning for maximum accuracy\n",
    "- **Strategy**: Unfreeze backbone, train all layers\n",
    "- **Duration**: 30 epochs (21-50)\n",
    "- **Learning Rate**: 0.0001 (10Ã— lower)\n",
    "- **Expected Accuracy**: ~87%\n",
    "\n",
    "### Why Fine-tuning Works for MobileNet-V2?\n",
    "1. **Preserved Optimizations**: Mobile optimizations remain intact with careful fine-tuning\n",
    "2. **Stable Architecture**: Inverted residuals provide stable gradient flow\n",
    "3. **Efficient Training**: Fewer parameters â†’ faster convergence\n",
    "4. **Good Generalization**: Pre-trained features adapt well to flower classification\n",
    "\n",
    "### Mobile-Specific Fine-tuning Considerations\n",
    "- **Lower Learning Rate**: Prevent disruption of mobile optimizations\n",
    "- **Gradual Unfreezing**: Could unfreeze layers gradually (optional)\n",
    "- **Regularization**: Weight decay prevents overfitting small model\n",
    "- **Monitoring**: Watch for degradation of mobile efficiency\n",
    "\n",
    "### Expected Improvements in Phase 2\n",
    "- **Accuracy Gain**: +7% improvement (80% â†’ 87%)\n",
    "- **Feature Adaptation**: Backbone learns flower-specific features\n",
    "- **End-to-end Optimization**: All layers work together\n",
    "- **Mobile Deployment Ready**: Maintains mobile optimization properties\n",
    "\n",
    "### Phase 2 Performance Targets\n",
    "- **Training Accuracy**: ~92%\n",
    "- **Validation Accuracy**: ~87%\n",
    "- **Training Time**: ~4-6 minutes additional\n",
    "- **Final Efficiency**: 24.9% accuracy per million parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tuning Training (Epochs 21-50)\n",
    "print(\"\\nğŸ”„ Starting Phase 2: Fine-tuning Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze backbone for fine-tuning\n",
    "unfreeze_backbone(model)\n",
    "\n",
    "# Reset optimizer with lower learning rate for fine-tuning\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "print(f\"ğŸ“‰ Learning rate reduced to 0.0001 for fine-tuning\")\n",
    "\n",
    "# Phase 2 training loop\n",
    "phase2_start_time = time.time()\n",
    "\n",
    "for epoch in range(21, 51):  # Epochs 21-50\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"ğŸŒŸ New best model! Validation accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch:2d}/50 | \"\n",
    "          f\"Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.2f}% | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping check (optional)\n",
    "    if epoch > 30 and val_acc < max(val_accuracies[-5:]) - 2:\n",
    "        print(f\"âš ï¸ Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "phase2_end_time = time.time()\n",
    "phase2_duration = phase2_end_time - phase2_start_time\n",
    "total_training_time = phase1_duration + phase2_duration\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ Phase 2 Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ğŸ¯ Final Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"â±ï¸ Phase 2 Training Time: {phase2_duration:.1f} seconds ({phase2_duration/60:.1f} minutes)\")\n",
    "print(f\"â±ï¸ Total Training Time: {total_training_time:.1f} seconds ({total_training_time/60:.1f} minutes)\")\n",
    "print(f\"ğŸ† Expected vs Actual: ~87% vs {best_val_acc:.2f}%\")\n",
    "\n",
    "# Performance improvement analysis\n",
    "phase1_best = max(val_accuracies[:20]) if len(val_accuracies) >= 20 else best_val_acc\n",
    "phase2_improvement = best_val_acc - phase1_best\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Training Progress Analysis:\")\n",
    "print(f\"   Phase 1 Best: {phase1_best:.2f}%\")\n",
    "print(f\"   Phase 2 Best: {best_val_acc:.2f}%\")\n",
    "print(f\"   Improvement: +{phase2_improvement:.2f}%\")\n",
    "print(f\"   Target Achievement: {best_val_acc/87*100:.1f}% of 87% target\")\n",
    "\n",
    "# Final performance metrics\n",
    "final_train_acc = train_accuracies[-1]\n",
    "final_val_acc = val_accuracies[-1]\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Performance Metrics:\")\n",
    "print(f\"   Final Training Accuracy: {final_train_acc:.2f}%\")\n",
    "print(f\"   Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
    "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Overfitting Gap: {final_train_acc - final_val_acc:.2f}%\")\n",
    "\n",
    "# Mobile efficiency analysis\n",
    "print(f\"\\nğŸ“± Mobile Efficiency Analysis:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Model Size: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "print(f\"   Accuracy per M Parameters: {best_val_acc/(total_params/1000000):.1f}%/M\")\n",
    "print(f\"   Training Speed: {len(train_dataset) * NUM_EPOCHS / total_training_time:.1f} images/second\")\n",
    "print(f\"   Inference Speed: Expected ~2-3ms per image\")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nâœ… Loaded best model for final evaluation\")\n",
    "    print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Model comparison summary\n",
    "print(f\"\\nğŸ† MobileNet-V2 vs Other Models:\")\n",
    "print(f\"   ResNet18: ~85% accuracy, 11.7M params\")\n",
    "print(f\"   ResNet50: ~88% accuracy, 25.6M params\") \n",
    "print(f\"   EfficientNet-B0: ~90% accuracy, 5.3M params\")\n",
    "print(f\"   EfficientNet-B3: ~92% accuracy, 12.2M params\")\n",
    "print(f\"   MobileNet-V2: {best_val_acc:.1f}% accuracy, {total_params/1000000:.1f}M params â­\")\n",
    "\n",
    "print(f\"\\nğŸš€ Mobile Deployment Advantages:\")\n",
    "print(f\"   âœ… Smallest model ({total_params/1000000:.1f}M parameters)\")\n",
    "print(f\"   âœ… Fastest training ({total_training_time/60:.1f} minutes)\")\n",
    "print(f\"   âœ… Highest efficiency ({best_val_acc/(total_params/1000000):.1f}% per M params)\")\n",
    "print(f\"   âœ… Mobile-optimized architecture\")\n",
    "print(f\"   âœ… Real-time inference capability\")\n",
    "print(f\"   âœ… Low memory footprint\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Training Complete! Ready for deployment analysis...\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: Results Visualization and Analysis\n",
    "\n",
    "### Performance Analysis\n",
    "Let's visualize the training progress and analyze MobileNet-V2's performance against all previous models:\n",
    "\n",
    "**Expected Results:**\n",
    "- **Phase 1**: ~80% validation accuracy (feature extraction only)\n",
    "- **Phase 2**: ~87% validation accuracy (fine-tuning)\n",
    "- **Training Time**: ~8-12 minutes (fastest among all models)\n",
    "- **Efficiency**: Best efficiency per parameter (24.9%/M)\n",
    "\n",
    "### Complete Model Comparison Summary\n",
    "| Model | Parameters | Accuracy | Efficiency | Training Time | Mobile Score |\n",
    "|-------|------------|----------|------------|---------------|--------------|\n",
    "| ResNet18 | 11.7M | ~85% | 7.3%/M | ~15-20min | â­â­ |\n",
    "| ResNet50 | 25.6M | ~88% | 3.4%/M | ~25-30min | â­ |\n",
    "| EfficientNet-B0 | 5.3M | ~90% | 17.0%/M | ~10-15min | â­â­â­â­ |\n",
    "| EfficientNet-B3 | 12.2M | ~92% | 7.5%/M | ~20-25min | â­â­â­ |\n",
    "| **MobileNet-V2** | **3.5M** | **~87%** | **24.9%/M** | **~8-12min** | **â­â­â­â­â­** |\n",
    "\n",
    "### Mobile Deployment Insights\n",
    "- **Best Efficiency**: Highest accuracy per parameter ratio\n",
    "- **Fastest Training**: Shortest total training time\n",
    "- **Smallest Model**: Lowest memory footprint\n",
    "- **Real-time Ready**: Optimized for mobile inference\n",
    "- **Balanced Performance**: Good accuracy with excellent efficiency\n",
    "\n",
    "### Architecture Innovation Impact\n",
    "- **Depthwise Separable Convolutions**: 88% parameter reduction\n",
    "- **Inverted Residuals**: Efficient gradient flow and feature reuse\n",
    "- **Mobile Optimization**: Purpose-built for edge deployment\n",
    "- **Scalable Design**: Width multiplier allows flexible sizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comprehensive training results for MobileNet-V2\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].axvline(x=20, color='gray', linestyle='--', alpha=0.7, label='Phase Transition')\n",
    "axes[0, 0].set_title('Training and Validation Loss (MobileNet-V2)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(epochs, train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].axvline(x=20, color='gray', linestyle='--', alpha=0.7, label='Phase Transition')\n",
    "axes[0, 1].set_title('Training and Validation Accuracy (MobileNet-V2)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time per epoch\n",
    "axes[1, 0].plot(epochs, epoch_times, 'g-', linewidth=2)\n",
    "axes[1, 0].axvline(x=20, color='gray', linestyle='--', alpha=0.7, label='Phase Transition')\n",
    "axes[1, 0].set_title('Training Time per Epoch (MobileNet-V2)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Complete model comparison - efficiency vs accuracy\n",
    "models = ['ResNet18', 'ResNet50', 'EfficientNet-B0', 'EfficientNet-B3', 'MobileNet-V2']\n",
    "params = [11.7, 25.6, 5.3, 12.2, 3.5]  # Million parameters\n",
    "accuracies = [85, 88, 90, 92, best_val_acc]  # Best validation accuracies\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'red']\n",
    "sizes = [100, 150, 120, 140, 200]  # Different sizes for emphasis\n",
    "\n",
    "# Create scatter plot: parameters vs accuracy\n",
    "scatter = axes[1, 1].scatter(params, accuracies, c=colors, s=sizes, alpha=0.7, edgecolors='black')\n",
    "axes[1, 1].set_title('Final Model Comparison: Parameters vs Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Parameters (Millions)')\n",
    "axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "axes[1, 1].set_xlim(0, 30)\n",
    "axes[1, 1].set_ylim(80, 95)\n",
    "\n",
    "# Add model labels with efficiency\n",
    "for i, model in enumerate(models):\n",
    "    efficiency = accuracies[i] / params[i]\n",
    "    axes[1, 1].annotate(f'{model}\\n({efficiency:.1f}%/M)', \n",
    "                       (params[i], accuracies[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', \n",
    "                       fontsize=8, ha='left')\n",
    "\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive final results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MOBILENET-V2 TRANSFER LEARNING FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ğŸ“Š Final Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"ğŸ“ˆ Expected: ~87%, Achieved: {best_val_acc:.2f}%\")\n",
    "print(f\"â±ï¸ Total Training Time: {total_training_time:.1f} seconds ({total_training_time/60:.1f} minutes)\")\n",
    "print(f\"ğŸ”§ Model Parameters: {total_params:,}\")\n",
    "print(f\"ğŸ“± Model Size: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "\n",
    "# Phase analysis\n",
    "phase1_acc = val_accuracies[19] if len(val_accuracies) >= 20 else val_accuracies[-1]\n",
    "phase2_acc = best_val_acc\n",
    "print(f\"\\nğŸ“Š Phase Analysis:\")\n",
    "print(f\"   Phase 1 (Feature Extraction): {phase1_acc:.2f}%\")\n",
    "print(f\"   Phase 2 (Fine-tuning): {phase2_acc:.2f}%\")\n",
    "print(f\"   Improvement: +{phase2_acc - phase1_acc:.2f}%\")\n",
    "\n",
    "# Complete model ranking by different metrics\n",
    "print(f\"\\nğŸ† FINAL MODEL RANKING ACROSS ALL METRICS:\")\n",
    "\n",
    "print(f\"\\nğŸ“Š By Accuracy (High to Low):\")\n",
    "model_accuracies = [\n",
    "    (\"EfficientNet-B3\", 92, 12.2),\n",
    "    (\"EfficientNet-B0\", 90, 5.3),\n",
    "    (\"ResNet50\", 88, 25.6),\n",
    "    (\"MobileNet-V2\", best_val_acc, 3.5),\n",
    "    (\"ResNet18\", 85, 11.7)\n",
    "]\n",
    "for i, (model, acc, params) in enumerate(sorted(model_accuracies, key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"   {i}. {model}: {acc:.1f}% ({params:.1f}M params)\")\n",
    "\n",
    "print(f\"\\nâš¡ By Efficiency (% per M params, High to Low):\")\n",
    "model_efficiency = [(name, acc/params) for name, acc, params in model_accuracies]\n",
    "for i, (model, eff) in enumerate(sorted(model_efficiency, key=lambda x: x[1], reverse=True), 1):\n",
    "    print(f\"   {i}. {model}: {eff:.1f}% per M params\")\n",
    "\n",
    "print(f\"\\nğŸ“± By Mobile Deployment Score:\")\n",
    "print(f\"   ğŸ¥‡ MobileNet-V2: {best_val_acc:.1f}% accuracy, {3.5:.1f}M params, {total_training_time/60:.1f}min\")\n",
    "print(f\"   ğŸ¥ˆ EfficientNet-B0: 90% accuracy, 5.3M params, ~12min\")\n",
    "print(f\"   ğŸ¥‰ EfficientNet-B3: 92% accuracy, 12.2M params, ~22min\")\n",
    "print(f\"   4th ResNet18: 85% accuracy, 11.7M params, ~18min\")\n",
    "print(f\"   5th ResNet50: 88% accuracy, 25.6M params, ~28min\")\n",
    "\n",
    "# Mobile deployment analysis\n",
    "print(f\"\\nğŸ“± MOBILE DEPLOYMENT ANALYSIS:\")\n",
    "print(f\"   ğŸ† Winner: MobileNet-V2\")\n",
    "print(f\"   â€¢ Smallest model: {total_params/1000000:.1f}M parameters\")\n",
    "print(f\"   â€¢ Fastest training: {total_training_time/60:.1f} minutes\")\n",
    "print(f\"   â€¢ Best efficiency: {best_val_acc/(total_params/1000000):.1f}% per M params\")\n",
    "print(f\"   â€¢ Real-time inference: ~2-3ms per image\")\n",
    "print(f\"   â€¢ Memory footprint: ~{total_params * 4 / 1024**2:.1f}MB\")\n",
    "print(f\"   â€¢ Battery friendly: Optimized for mobile CPUs\")\n",
    "\n",
    "# Architecture innovation impact\n",
    "print(f\"\\nğŸ”¬ ARCHITECTURE INNOVATION IMPACT:\")\n",
    "print(f\"   ğŸ’¡ Depthwise Separable Convolutions:\")\n",
    "print(f\"     â€¢ Parameter reduction: 88% fewer than standard convolutions\")\n",
    "print(f\"     â€¢ Computational efficiency: 8-9Ã— fewer multiply-adds\")\n",
    "print(f\"     â€¢ Memory efficiency: Lower intermediate activations\")\n",
    "print(f\"   ğŸ’¡ Inverted Residual Blocks:\")\n",
    "print(f\"     â€¢ Efficient feature reuse with expand-depthwise-project\")\n",
    "print(f\"     â€¢ Stable gradient flow for mobile architectures\")\n",
    "print(f\"     â€¢ Linear bottlenecks prevent information loss\")\n",
    "\n",
    "# Practical deployment recommendations\n",
    "print(f\"\\nğŸš€ PRACTICAL DEPLOYMENT RECOMMENDATIONS:\")\n",
    "print(f\"   ğŸ“± Use Cases:\")\n",
    "print(f\"     â€¢ Mobile flower identification apps\")\n",
    "print(f\"     â€¢ Edge device botanical monitoring\")\n",
    "print(f\"     â€¢ IoT garden management systems\")\n",
    "print(f\"     â€¢ Real-time plant classification\")\n",
    "print(f\"   âš™ï¸ Optimization Strategies:\")\n",
    "print(f\"     â€¢ Quantization: INT8 for 4Ã— inference speedup\")\n",
    "print(f\"     â€¢ Pruning: Remove redundant connections\")\n",
    "print(f\"     â€¢ Knowledge distillation: Train even smaller models\")\n",
    "print(f\"     â€¢ Hardware acceleration: Utilize mobile NPUs\")\n",
    "\n",
    "# Course completion summary\n",
    "print(f\"\\nğŸ“ TRANSFER LEARNING COURSE COMPLETION SUMMARY:\")\n",
    "print(f\"   âœ… 5 Model Architectures Mastered:\")\n",
    "print(f\"     1. ResNet18: Residual learning foundations\")\n",
    "print(f\"     2. ResNet50: Deep network scaling\")\n",
    "print(f\"     3. EfficientNet-B0: Compound scaling introduction\")\n",
    "print(f\"     4. EfficientNet-B3: Systematic scaling mastery\")\n",
    "print(f\"     5. MobileNet-V2: Mobile optimization excellence\")\n",
    "print(f\"   âœ… Key Concepts Learned:\")\n",
    "print(f\"     â€¢ Transfer learning strategies\")\n",
    "print(f\"     â€¢ Progressive training techniques\")\n",
    "print(f\"     â€¢ Architecture trade-offs\")\n",
    "print(f\"     â€¢ Mobile deployment considerations\")\n",
    "print(f\"     â€¢ Efficiency optimization methods\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ‰ CONGRATULATIONS! Transfer Learning Course Complete! ğŸ‰\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Course Completion: Key Takeaways and Final Summary\n",
    "\n",
    "### âœ… What We've Accomplished\n",
    "1. **Mobile Architecture Mastery**: Successfully implemented MobileNet-V2, the most efficient CNN for mobile deployment\n",
    "2. **Complete Model Comparison**: Analyzed 5 different architectures across multiple metrics\n",
    "3. **Transfer Learning Excellence**: Mastered progressive training strategies and fine-tuning techniques\n",
    "4. **Practical Deployment Skills**: Learned mobile optimization and real-world deployment considerations\n",
    "\n",
    "### ğŸ” MobileNet-V2 Key Insights\n",
    "- **Architecture Innovation**: Depthwise separable convolutions and inverted residuals revolutionize mobile AI\n",
    "- **Efficiency Champion**: Best accuracy per parameter ratio (24.9%/M) among all models tested\n",
    "- **Training Speed**: Fastest training time (~8-12 minutes) enables rapid iteration\n",
    "- **Mobile Ready**: Purpose-built for edge deployment with real-time inference capability\n",
    "\n",
    "### ğŸ“Š Complete Course Model Comparison\n",
    "| Lesson | Model | Innovation | Accuracy | Parameters | Efficiency | Use Case |\n",
    "|--------|--------|------------|----------|------------|------------|----------|\n",
    "| 3 | ResNet18 | Residual Learning | ~85% | 11.7M | 7.3%/M | Learning foundations |\n",
    "| 4 | ResNet50 | Deep Networks | ~88% | 25.6M | 3.4%/M | Server deployment |\n",
    "| 5 | EfficientNet-B0 | Compound Scaling | ~90% | 5.3M | 17.0%/M | Balanced efficiency |\n",
    "| 6 | EfficientNet-B3 | Systematic Scaling | ~92% | 12.2M | 7.5%/M | High accuracy needs |\n",
    "| **7** | **MobileNet-V2** | **Mobile Optimization** | **~87%** | **3.5M** | **24.9%/M** | **Mobile/Edge deployment** |\n",
    "\n",
    "### ğŸ† Architecture Evolution Understanding\n",
    "1. **ResNet Series**: Introduced residual learning for training very deep networks\n",
    "2. **EfficientNet Series**: Demonstrated compound scaling for systematic performance improvements\n",
    "3. **MobileNet-V2**: Optimized every component for mobile efficiency without sacrificing too much accuracy\n",
    "\n",
    "### ğŸ“± Mobile AI Deployment Mastery\n",
    "- **Quantization**: Reduce model precision for 4Ã— speedup\n",
    "- **Pruning**: Remove redundant connections for smaller models\n",
    "- **Knowledge Distillation**: Train compact student models from larger teachers\n",
    "- **Hardware Acceleration**: Leverage mobile NPUs and specialized chips\n",
    "\n",
    "### ğŸš€ Real-World Applications\n",
    "**Immediate Deployment Scenarios:**\n",
    "- **Mobile Apps**: Real-time flower identification with offline capability\n",
    "- **Edge Devices**: Botanical monitoring systems for gardens and farms\n",
    "- **IoT Systems**: Smart plant care with minimal power consumption\n",
    "- **Web Applications**: Fast inference without GPU requirements\n",
    "\n",
    "### ğŸ“ Skills Developed\n",
    "**Technical Expertise:**\n",
    "- Transfer learning strategy design and implementation\n",
    "- Progressive training with phase transitions\n",
    "- Architecture analysis and performance optimization\n",
    "- Mobile deployment consideration and optimization\n",
    "\n",
    "**Practical Knowledge:**\n",
    "- Model selection based on deployment constraints\n",
    "- Trade-off analysis between accuracy and efficiency\n",
    "- Performance benchmarking and comparison\n",
    "- Real-world deployment planning\n",
    "\n",
    "### ğŸ’¡ Next Steps and Advanced Topics\n",
    "**Immediate Applications:**\n",
    "- Deploy MobileNet-V2 to your mobile application\n",
    "- Experiment with different width multipliers (0.5Ã—, 0.75Ã—)\n",
    "- Try quantization and pruning for further optimization\n",
    "- Implement ensemble methods combining multiple models\n",
    "\n",
    "**Advanced Exploration:**\n",
    "- Custom architecture design using learned principles\n",
    "- Neural Architecture Search (NAS) for automated optimization\n",
    "- Cross-platform deployment (TensorFlow Lite, ONNX)\n",
    "- Model compression and acceleration techniques\n",
    "\n",
    "### ğŸŒŸ Course Achievement Unlocked\n",
    "**ğŸ‰ Congratulations! You have successfully completed the Transfer Learning for Computer Vision course!**\n",
    "\n",
    "You now possess the knowledge and skills to:\n",
    "- âœ… Choose the right CNN architecture for any deployment scenario\n",
    "- âœ… Implement transfer learning effectively across different domains\n",
    "- âœ… Optimize models for mobile and edge deployment\n",
    "- âœ… Balance accuracy and efficiency based on constraints\n",
    "- âœ… Deploy computer vision models in production environments\n",
    "\n",
    "**Continue your AI journey with confidence! ğŸš€ğŸ“±ğŸ¤–**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}